{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSESSMENT\n",
    "    Data: 1. articles.csv - contains 1000 articles that cuta cross      various fields\n",
    "    Data: 2. google-phd-program.txt - a short article on google's proposed new PhD program.\n",
    "    Data: 3. comment.txt - A comment to google article.\n",
    "    \n",
    "    Data: 4. oliver-twist-0-9.txt - Chapters 1-9 of the charles dickens popular book Oliver Twist\n",
    "    \n",
    "    \n",
    "    Instruction: Read the command in each cell and write code to give the same output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:59:52.291477Z",
     "start_time": "2019-10-20T11:59:45.705831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import standrad libraries to injest the CSV file and read in the CSV file\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:00:24.987807Z",
     "start_time": "2019-10-20T12:00:24.845710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does it mean to be successful? Perhaps, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Much has been made of Africa&amp;rsquo;s unfavorab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A few days after a Damaris Wambui Kamau and he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\nJust a few hours after their release o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IPOA is investigating the murder of four peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles\n",
       "0   1  What does it mean to be successful? Perhaps, o...\n",
       "1   2  Much has been made of Africa&rsquo;s unfavorab...\n",
       "2   3  A few days after a Damaris Wambui Kamau and he...\n",
       "3   4  \\n\\n\\n\\nJust a few hours after their release o...\n",
       "4   5  IPOA is investigating the murder of four peopl..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the email addresses contained in entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:04:03.514413Z",
     "start_time": "2019-10-20T12:04:02.343633Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# import standard package\n",
    "import re \n",
    "\n",
    "# Enter your regex pattern here. This may take several tries!\n",
    "\n",
    "email_list = []\n",
    "pattern = r'[\\w]+@[\\w]+.\\w{2,}'\n",
    "for row in data['articles']:\n",
    "    emails = re.findall(pattern, row)\n",
    "    if emails: [email_list.append(email) for email in emails]\n",
    "\n",
    "print(len(email_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:04:25.328657Z",
     "start_time": "2019-10-20T12:04:16.499526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:05:12.299626Z",
     "start_time": "2019-10-20T12:05:11.653171Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an article object from the 'google-phd-program.txt' file\n",
    "\n",
    "with open('google-phd-program.txt', encoding='utf8') as f:\n",
    "    article = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:05:29.800667Z",
     "start_time": "2019-10-20T12:05:29.779653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many sentences are conatined in the file\n",
    "sents = [sent for sent in article.sents]\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instead of receiving a boring paper diploma at the end of their PhDs, Google PhD graduates receive a priceless note handwritten by Jeff Dean on a napkin that says, “You have a PhD now.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the FOURTEENTH sentence\n",
    "\n",
    "print(sents[13].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instead         ADV   advmod     instead        \n",
      "of              ADP   prep       of             \n",
      "receiving       VERB  pcomp      receive        \n",
      "a               DET   det        a              \n",
      "boring          ADJ   amod       boring         \n",
      "paper           NOUN  compound   paper          \n",
      "diploma         NOUN  dobj       diploma        \n",
      "at              ADP   prep       at             \n",
      "the             DET   det        the            \n",
      "end             NOUN  pobj       end            \n",
      "of              ADP   prep       of             \n",
      "their           PRON  poss       -PRON-         \n",
      "PhDs            PROPN pobj       PhDs           \n",
      ",               PUNCT punct      ,              \n",
      "Google          PROPN compound   Google         \n",
      "PhD             NOUN  compound   phd            \n",
      "graduates       VERB  nsubj      graduate       \n",
      "receive         VERB  ROOT       receive        \n",
      "a               DET   det        a              \n",
      "priceless       ADJ   amod       priceless      \n",
      "note            NOUN  dobj       note           \n",
      "handwritten     ADJ   acl        handwritten    \n",
      "by              ADP   agent      by             \n",
      "Jeff            PROPN compound   Jeff           \n",
      "Dean            PROPN pobj       Dean           \n",
      "on              ADP   prep       on             \n",
      "a               DET   det        a              \n",
      "napkin          NOUN  pobj       napkin         \n",
      "that            PRON  nsubj      that           \n",
      "says            VERB  relcl      say            \n",
      ",               PUNCT punct      ,              \n",
      "“               PUNCT punct      \"              \n",
      "You             PRON  nsubj      -PRON-         \n",
      "have            AUX   ccomp      have           \n",
      "a               DET   det        a              \n",
      "PhD             NOUN  dobj       phd            \n",
      "now             ADV   advmod     now            \n",
      ".               PUNCT punct      .              \n",
      "”               PUNCT punct      \"              \n",
      "\n",
      "               SPACE            \n",
      "              \n"
     ]
    }
   ],
   "source": [
    "# For each token in the sentence above, print its text, POS tag,\n",
    "# dep tag and lemma. \n",
    "#Have values line up in columns like in the print output.\n",
    "\n",
    "for token in sents[13]:\n",
    "    print(f'{token.text:{15}} {token.pos_:{5}} {token.dep_:{10}} {token.lemma_:{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Brain, DeepMind    ORG   Companies, agencies, institutions, etc.\n",
      "Stanford                  GPE   Countries, cities, states\n",
      "Berkeley                  GPE   Countries, cities, states\n",
      "MIT                       ORG   Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Extract entites in the SECOND sentence\n",
    "\n",
    "for ent in sents[1].ents:\n",
    "    print(f'{ent.text:{25}} {ent.label_:{5}} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a matcher called 'AI' that finds all occurrences\n",
    "# of the phrase \"Artificial Intelligence\" and word 'AI' in the article\n",
    "\n",
    "# First Run this standard import and call\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the patterns and add it to matcher:\n",
    "\n",
    "pattern = [{'LOWER': 'ai'}]\n",
    "pattern1 = [{'LOWER': 'artificial'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'intelligence'}]\n",
    "\n",
    "matcher.add('AI', None, pattern, pattern1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5530044837203964789, 12, 13), (5530044837203964789, 47, 49), (5530044837203964789, 138, 139), (5530044837203964789, 155, 157), (5530044837203964789, 285, 286), (5530044837203964789, 408, 409), (5530044837203964789, 421, 422), (5530044837203964789, 458, 459)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches called \"found_matches\" and print the list:\n",
    "\n",
    "found_matches = matcher(article)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a FUNCTION that takes in a index and prints the sentence\n",
    "#in which the match at that index is found\n",
    "\n",
    "def print_match_sent(index):\n",
    "    for sent in sents:\n",
    "        if found_matches[index][1] < sent.end:\n",
    "            return print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“It’s extremely difficult to find people with talent in Machine Learning and Artificial Intelligence these days, and it pained us to see so many of our amazing interns decline our return offers to go back to school for PhDs.\n"
     ]
    }
   ],
   "source": [
    "# print the sentence where the FOURTH match is \n",
    "print_match_sent(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the meantime, Google is already planning for the next way to find top AI talent.\n"
     ]
    }
   ],
   "source": [
    "# print the sentence where the last match is \n",
    "print_match_sent(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84. ADJ   :38\n",
      "85. ADP   :48\n",
      "86. ADV   :19\n",
      "87. AUX   :23\n",
      "89. CCONJ :11\n",
      "90. DET   :30\n",
      "91. INTJ  :1\n",
      "92. NOUN  :110\n",
      "93. NUM   :3\n",
      "94. PART  :15\n",
      "95. PRON  :26\n",
      "96. PROPN :38\n",
      "97. PUNCT :62\n",
      "98. SCONJ :10\n",
      "100. VERB  :48\n",
      "103. SPACE :6\n"
     ]
    }
   ],
   "source": [
    "# Provide a frequency list of POS tags from the entire article\n",
    "\n",
    "POS_counts = article.count_by(spacy.attrs.POS)\n",
    "for k,v in sorted(POS_counts.items()):\n",
    "    print(f'{k}. {article.vocab[k].text:{5}} :{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/488 = 22.54%\n"
     ]
    }
   ],
   "source": [
    "# What percentage of tokens are nouns?\n",
    "# I used 92 in my calculation because Noun has a key value of 92 for my spacy version, so I am changing it to 91 now because \n",
    "# your spacy version assigns key value of 91 to Noun\n",
    "\n",
    "percent = 100*POS_counts[91]/len(article)\n",
    "print(f'{POS_counts[91]}/{len(article)} = {percent:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In a move that is completely unsurprising to many, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s AI research division has announced that they are issuing \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    PhD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " degrees to select employees.</br>Industry research organizations like \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google Brain, DeepMind\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and FAIR are well known as heavy hitters in the artificial intelligence research community, publishing as many papers (if not more) as academic institutions like \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Stanford\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Berkeley\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    MIT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Many top professors from academia have migrated over to industry research labs as well, sacrificing the security of academic tenure for fat stacks of money. Although \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has previously experimented with research residencies, this is the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " time that they have issued postgraduate degrees.</br>According to a representative, the tech giant decided to issue PhDs in order to attract scarce AI talent. “It’s extremely difficult to find people with talent in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Machine Learning and Artificial Intelligence\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    these days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and it pained us to see so many of our amazing interns decline our return offers to go back to school for \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    PhDs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ". With the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " PhD program, we can continue to get the best talent while supporting our student-employees with world class resources and technology.”</br>For graduate students, the \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google PhD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " program is a sweet deal. There is already a well-known pipeline where top graduate students do their PhDs at a university, intern at industry research groups during the summer, and then sign on full time after graduation with the promise of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    million-dollar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " compensation packages and the freedom to set their own research agendas.</br>However, aspiring AI researchers must still struggle through the difficulties of academia: \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    four to six years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of low pay, begging for funding, and advisors who can be sadistic, micromanaging, or absent. In contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " offers researchers massive computational resources and motivates their \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    PhD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " advisors with a potent combination of vegan snacks, kombucha, and equity refreshers. Instead of receiving a boring paper diploma at the end of their \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    PhDs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google PhD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " graduates receive a priceless note handwritten by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Jeff Dean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " on a napkin that says, “You have a \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    PhD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " now.”</br>Other tech companies are scrambling to build their own research groups so they are not left behind in the “AI revolution.” Halting Problem reached out to a representative from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Salesforce AI Research\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " who said, “What? \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is doing it? Well, guess we are too.”</br>In the meantime, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is already planning for the next way to find top AI talent. According to a knowledgeable source, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is in talks with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Khan Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to create a “machine learning kindergarten” program that teaches children \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Tensorflow\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the named entity visualization for the entire article\n",
    "from spacy import displacy\n",
    "displacy.render(article, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL : Which named entity do you think is not well tagged\n",
    "\n",
    "1. \"Stanford\" and \"Berkeley\" \n",
    "2. \"Machine Learning and Artificial Intelligence\"\n",
    "3. \"PhD\" and \"PhDs\"\n",
    "\n",
    "### Answer here and why:\n",
    "\n",
    "1. \"Stanford\" and \"Berkeley\": were used in the context of Academic institutions inside this article but are represented instead as GPE(Countries, cities, states)\n",
    "2. \"Machine Learning and Artificial Intelligence\": were also used in the context of a field / specialization but was interpreted as ORG(Companies, agencies, institutions, etc.)\n",
    "3. \"PhD\" and \"PhDs\" was used in the context of honorary award / Degree but are represented nstead as WORK_OF_ART(Titles of books, songs, etc.) / NORP - Nationalities or religious or political groups / PRODUCT(Objects, vehicles, foods, etc.) / PERSON(People, including fictional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform VADER Sentiment Analysis on the Comment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:18:25.793774Z",
     "start_time": "2019-10-20T12:17:58.973672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import SentimentIntensityAnalyzer and create an sid object\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# kindly ignore the warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:22:38.344050Z",
     "start_time": "2019-10-20T12:22:38.329041Z"
    }
   },
   "outputs": [],
   "source": [
    "# using 'open' read in the comment.txt file \n",
    "with open('comment.txt') as c:\n",
    "    comment = c.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:24:03.883266Z",
     "start_time": "2019-10-20T12:24:03.857248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.022, 'neu': 0.809, 'pos': 0.17, 'compound': 0.9186}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the sid scores for the comment\n",
    "sid.polarity_scores(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T12:26:30.109509Z",
     "start_time": "2019-10-20T12:26:30.091496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write fucntion to take in a comment and returns whether it was \n",
    "# \"Positive\", \"Negative\" or \"Neutral\"\n",
    "\n",
    "def comment_sentiment(comment):\n",
    "    scores = sid.polarity_scores(comment)\n",
    "    if scores['compound'] == 0:\n",
    "        return 'Neutral'\n",
    "    elif scores['compound'] > 0:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:30:46.525316Z",
     "start_time": "2019-10-20T13:30:46.499298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncomment = 'It is good to know that the outcomes of the Google PhD program are alligning with the calculated objectives for\\ngoogle except that I just hope the compensation packages is not the motivating factor to acquiring the assumed great talent\\ninstead of the right talents with the passion'\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a comment of the article (AFTER READING IT please) as one continuous string or (multiple sentences are ok)\n",
    "comment = 'It is good to know that the outcomes of the Google PhD program are alligning with the calculated objectives for \\\n",
    "google except that I just hope the compensation packages is not the motivating factor to acquiring the assumed great talent \\\n",
    "instead of the right talents with the passion'\n",
    "\n",
    "# As an example here is my own comment, please do not use mine.\n",
    "'''\n",
    "comment = 'It is good to know that the outcomes of the Google PhD program are alligning with the calculated objectives for\n",
    "google except that I just hope the compensation packages is not the motivating factor to acquiring the assumed great talent\n",
    "instead of the right talents with the passion'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:31:05.349207Z",
     "start_time": "2019-10-20T13:31:05.324191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function on your comment above:\n",
    "comment_sentiment(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with LDA and NMF\n",
    "First we'd look at LDA then NMF and at the end see how both pan out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for LDA\n",
    "Task: Use Count Vectorization to create a vectorized document sparse matrix. Use a max_df=0.9 and min_df=2 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:43:50.523669Z",
     "start_time": "2019-10-20T13:43:49.771467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required packages i.e Countvectorizer, LDA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:51:38.969536Z",
     "start_time": "2019-10-20T13:51:36.598961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1001x10339 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 109044 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the CV object, Fit and transform the countvectorizer \n",
    "# on the articles using the params giving above\n",
    "article = pd.read_csv(\"articles.csv\")\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(article[\"articles\"])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:54:17.211882Z",
     "start_time": "2019-10-20T13:53:46.125051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=10, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit LDA object to the sparse matrix fron the count vectorizer\n",
    "# Extracting 10 TOPICS with a RANDOM STATE of 10\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=10,random_state=10)\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T13:57:24.193254Z",
     "start_time": "2019-10-20T13:57:15.085198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 20 WORDS FOR TOPIC #0\n",
      "['nigerian', 'police', 'south', 'statement', 'federal', 'according', 'states', 'national', 'security', 'buhari', 'people', 'nigeria', 'state', 'country', 'government', 'rsquo', 'president', 'rdquo', 'said', 'ldquo']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #1\n",
      "['general', 'rsquo', 'capital', 'market', 'business', 'programme', 'new', '2019', 'sec', 'rdquo', 'nigerian', 'nigeria', 'african', 'development', 'commission', 'company', '000', 'ldquo', 'africa', 'said']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #2\n",
      "['officers', '000', 'year', 'air', 'airlines', 'million', 'new', 'according', 'army', 'police', 'oil', 'project', 'government', 'state', 'nigeria', 'nigerian', 'rdquo', 'rsquo', 'ldquo', 'said']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #3\n",
      "['year', 'respondent', 'server', 'candidate', 'president', 'rdquo', 'apc', 'party', 'national', 'plastic', 'atiku', 'results', 'said', 'buhari', 'inec', 'rsquo', 'pdp', 'election', 'ldquo', 'presidential']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #4\n",
      "['energy', 'increase', 'africa', 'government', 'new', 'tax', 'market', 'agency', 'development', 'state', 'nnpc', 'national', 'million', 'according', 'gas', 'nigeria', 'rdquo', 'rsquo', 'ldquo', 'said']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #5\n",
      "['match', 'club', 'egypt', 'group', 'rohr', '2019', 'time', 'ldquo', 'world', 'game', 'rdquo', 'nations', 'players', 'africa', 'nigeria', 'team', 'rsquo', 'super', 'cup', 'eagles']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #6\n",
      "['parties', 'nigeria', 'buhari', 'pdp', 'people', 'political', 'presidential', 'court', 'state', 'inec', 'electoral', 'commission', 'party', 'president', 'elections', 'rsquo', 'rdquo', 'ldquo', 'said', 'election']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #7\n",
      "['2019', 'stock', 'banks', 'years', 'report', '2018', 'said', 'shares', 'company', 'year', 'cbn', 'mtn', 'market', 'according', 'exchange', 'billion', 'percent', 'million', 'nigeria', 'bank']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #8\n",
      "['world', 'child', 'new', 'singer', 'instagram', 'love', 'don', 'man', 'know', 'years', 'time', 'life', 'people', 'just', 'like', 'said', 'lsquo', 'rdquo', 'ldquo', 'rsquo']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #9\n",
      "['minute', 'manchester', 'city', 'second', 'half', 'minutes', 'win', 'premier', 'arsenal', 'champions', 'united', 'team', 'game', 'club', 'season', 'goals', 'goal', 'chelsea', 'rsquo', 'league']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the the TOP 20 words of the all Topics \n",
    "\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 20 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T14:44:20.612328Z",
     "start_time": "2019-10-20T14:44:20.603326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the sparse data on the fitted LDA object\n",
    "\n",
    "topic_results = LDA.transform(dtm)\n",
    "\n",
    "# Create a new column called 'Topic_lda' and save the Topic id's in it\n",
    "\n",
    "article['Topic_lda'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T14:44:37.917935Z",
     "start_time": "2019-10-20T14:44:37.870904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>Topic_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does it mean to be successful? Perhaps, o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Much has been made of Africa&amp;rsquo;s unfavorab...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A few days after a Damaris Wambui Kamau and he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\nJust a few hours after their release o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IPOA is investigating the murder of four peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles  Topic_lda\n",
       "0   1  What does it mean to be successful? Perhaps, o...          3\n",
       "1   2  Much has been made of Africa&rsquo;s unfavorab...          4\n",
       "2   3  A few days after a Damaris Wambui Kamau and he...          0\n",
       "3   4  \\n\\n\\n\\nJust a few hours after their release o...          0\n",
       "4   5  IPOA is investigating the murder of four peopl...          0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataframe now\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:00:33.077115Z",
     "start_time": "2019-10-20T15:00:33.052097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a label dictionary for each topic, kinda like mine,\n",
    "#please go through the words in each topic to figure out a fitting \n",
    "#label for that topic and create a dictionary mapping each topic id \n",
    "#to its corresponding label (which is at your discretion)\n",
    "\n",
    "label_dict = {\n",
    "    0:'security',\n",
    "    1:'economy',\n",
    "    2:'economy',\n",
    "    3:'election',\n",
    "    4:'economy',\n",
    "    5:'football',\n",
    "    6:'election',\n",
    "    7:'stock',\n",
    "    8:'social',\n",
    "    9:'football'\n",
    "}\n",
    "\n",
    "# there seem to be just 6 unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:02:55.118103Z",
     "start_time": "2019-10-20T15:02:54.724842Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>Topic_lda</th>\n",
       "      <th>lda_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does it mean to be successful? Perhaps, o...</td>\n",
       "      <td>3</td>\n",
       "      <td>election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Much has been made of Africa&amp;rsquo;s unfavorab...</td>\n",
       "      <td>4</td>\n",
       "      <td>economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A few days after a Damaris Wambui Kamau and he...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\nJust a few hours after their release o...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IPOA is investigating the murder of four peopl...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles  Topic_lda lda_labels\n",
       "0   1  What does it mean to be successful? Perhaps, o...          3   election\n",
       "1   2  Much has been made of Africa&rsquo;s unfavorab...          4    economy\n",
       "2   3  A few days after a Damaris Wambui Kamau and he...          0   security\n",
       "3   4  \\n\\n\\n\\nJust a few hours after their release o...          0   security\n",
       "4   5  IPOA is investigating the murder of four peopl...          0   security"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column called 'lda_labels' based on the map created above\n",
    "article['lda_labels'] = article['Topic_lda'].map(label_dict)\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NMF\n",
    "Task: Use TF-IDF Vectorization to create a vectorized document term matrix. Use a max_df=0.9 and min_df=2 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:03:18.808524Z",
     "start_time": "2019-10-20T15:03:18.792513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required packages i.e tfidfvectorizer, NMF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:05:24.880301Z",
     "start_time": "2019-10-20T15:05:22.952615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1001x10339 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 109044 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the tfidfvectorizer on the articles using the params giving above\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = tfidf.fit_transform(article[\"articles\"])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:08:11.899278Z",
     "start_time": "2019-10-20T15:08:03.436480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=10, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit an NMF instance to the sparse matrix from the tfidfvectorizer\n",
    "# Extracting 10 topics with a random state of 10\n",
    "\n",
    "nmf_model = NMF(n_components=10,random_state=10)\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:10:41.838260Z",
     "start_time": "2019-10-20T15:10:34.675797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 20 WORDS FOR TOPIC #0\n",
      "['national', 'administration', 'governors', 'federal', 'herdsmen', 'army', 'north', 'military', 'nigerians', 'people', 'rdquo', 'nigeria', 'security', 'said', 'buhari', 'state', 'country', 'ldquo', 'government', 'president']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #1\n",
      "['obi', 'match', 'burundi', 'injury', 'ighalo', 'coach', 'group', 'gernot', 'afcon', 'nigeria', 'egypt', 'mikel', 'nations', 'players', 'team', 'africa', 'rohr', 'cup', 'super', 'eagles']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #2\n",
      "['official', 'long', 'just', 'singer', 'man', 'songs', 'people', 'really', 'new', 'want', 'time', 'like', 'feel', 'world', 'don', 've', 'music', 'hellip', 'lsquo', 'rsquo']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #3\n",
      "['oil', 'plc', 'board', 'investors', 'million', 'said', 'percent', 'securities', 'billion', 'nigeria', 'nse', 'stock', 'commission', 'sec', 'shares', 'listing', 'exchange', 'market', 'mtn', 'company']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #4\n",
      "['goals', 'window', 'appearances', 'old', 'deal', 'city', 'summer', 'international', 'turkish', 'fenerbahce', 'player', 'premier', 'year', 'transfer', 'loan', 'moses', 'league', 'club', 'season', 'chelsea']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #5\n",
      "['ldquo', 'abubakar', 'votes', 'democratic', 'national', 'said', 'state', 'president', 'candidate', 'commission', 'apc', 'buhari', 'atiku', 'presidential', 'electoral', 'pdp', 'elections', 'party', 'inec', 'election']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #6\n",
      "['score', 'arsenal', 'scored', 'liverpool', 'united', 'falcons', 'team', 'second', 'half', 'final', 'minutes', 'goals', 'match', 'barcelona', 'champions', 'win', 'minute', 'league', 'goal', 'game']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #7\n",
      "['time', 'people', 'old', 'like', 'love', 'god', 'just', 'know', 'wrote', 'police', 'instagram', 'actor', 'children', 'life', 'ndash', 'family', 'rsquo', 'said', 'rdquo', 'ldquo']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #8\n",
      "['growth', 'merger', 'loans', 'development', 'nexim', 'sector', 'central', 'billion', 'customers', 'said', 'banking', 'plc', 'emefiele', 'nigeria', 'access', 'diamond', 'financial', 'banks', 'cbn', 'bank']\n",
      "\n",
      "\n",
      "THE TOP 20 WORDS FOR TOPIC #9\n",
      "['known', 'boys', 'fashola', 'nigerian', 'davido', 'musician', 'ldquo', 'guilty', 'money', 'jail', 'justice', 'bail', 'crimes', 'singer', 'fraud', 'court', 'yahoo', 'naira', 'efcc', 'marley']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the the TOP 20 words of the all Topics \n",
    "\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 20 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:13:32.781027Z",
     "start_time": "2019-10-20T15:13:32.653948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>Topic_lda</th>\n",
       "      <th>lda_labels</th>\n",
       "      <th>Topic_nmf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does it mean to be successful? Perhaps, o...</td>\n",
       "      <td>3</td>\n",
       "      <td>election</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Much has been made of Africa&amp;rsquo;s unfavorab...</td>\n",
       "      <td>4</td>\n",
       "      <td>economy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A few days after a Damaris Wambui Kamau and he...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\nJust a few hours after their release o...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IPOA is investigating the murder of four peopl...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles  Topic_lda  \\\n",
       "0   1  What does it mean to be successful? Perhaps, o...          3   \n",
       "1   2  Much has been made of Africa&rsquo;s unfavorab...          4   \n",
       "2   3  A few days after a Damaris Wambui Kamau and he...          0   \n",
       "3   4  \\n\\n\\n\\nJust a few hours after their release o...          0   \n",
       "4   5  IPOA is investigating the murder of four peopl...          0   \n",
       "\n",
       "  lda_labels  Topic_nmf  \n",
       "0   election          2  \n",
       "1    economy          3  \n",
       "2   security          7  \n",
       "3   security          7  \n",
       "4   security          7  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforn the data on the fitted NMF instance\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "\n",
    "# Create a new column called Topic_nmf and save the Topic id's in it\n",
    "\n",
    "article['Topic_nmf'] = topic_results.argmax(axis=1)\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:18:27.908071Z",
     "start_time": "2019-10-20T15:18:27.888062Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using the same set of labels now rearrange them to suit the topics \n",
    "#i.e if topic 7 for lda was labeled election, but now topic 7 looks \n",
    "#to be social, then modify the labels_dict accordingly to suit NMF\n",
    "\n",
    "label_dict = {\n",
    "    0:'security',\n",
    "    1:'football',\n",
    "    2:'social',\n",
    "    3:'stock',\n",
    "    4:'football',\n",
    "    5:'election',\n",
    "    6:'football',\n",
    "    7:'social',\n",
    "    8:'economy',\n",
    "    9:'social'\n",
    "}\n",
    "# maintaining the six unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T15:20:56.439395Z",
     "start_time": "2019-10-20T15:20:56.370348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>Topic_lda</th>\n",
       "      <th>lda_labels</th>\n",
       "      <th>Topic_nmf</th>\n",
       "      <th>nmf_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does it mean to be successful? Perhaps, o...</td>\n",
       "      <td>3</td>\n",
       "      <td>election</td>\n",
       "      <td>2</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Much has been made of Africa&amp;rsquo;s unfavorab...</td>\n",
       "      <td>4</td>\n",
       "      <td>economy</td>\n",
       "      <td>3</td>\n",
       "      <td>stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A few days after a Damaris Wambui Kamau and he...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n\\n\\nJust a few hours after their release o...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IPOA is investigating the murder of four peopl...</td>\n",
       "      <td>0</td>\n",
       "      <td>security</td>\n",
       "      <td>7</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles  Topic_lda  \\\n",
       "0   1  What does it mean to be successful? Perhaps, o...          3   \n",
       "1   2  Much has been made of Africa&rsquo;s unfavorab...          4   \n",
       "2   3  A few days after a Damaris Wambui Kamau and he...          0   \n",
       "3   4  \\n\\n\\n\\nJust a few hours after their release o...          0   \n",
       "4   5  IPOA is investigating the murder of four peopl...          0   \n",
       "\n",
       "  lda_labels  Topic_nmf nmf_labels  \n",
       "0   election          2     social  \n",
       "1    economy          3      stock  \n",
       "2   security          7     social  \n",
       "3   security          7     social  \n",
       "4   security          7     social  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column called lda_labels based on the map created above\n",
    "article['nmf_labels'] = article['Topic_nmf'].map(label_dict)\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:44:45.365106Z",
     "start_time": "2019-10-21T09:44:45.320077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many of the articles share the same label \n",
    "\n",
    "equal_column_labels = article[\"lda_labels\"] == article[\"nmf_labels\"]\n",
    "equal_column_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:38:34.386885Z",
     "start_time": "2019-10-21T09:38:34.278826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.94%\n"
     ]
    }
   ],
   "source": [
    "# What % of our articles were labelled with the same label by both topic models\n",
    "print(f'{(equal_column_labels.sum() / len(article))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Challenge: Text Generation with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:08:40.900475Z",
     "start_time": "2019-10-21T13:08:40.882465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the oliver-twist-0-9.txt text\n",
    "with open(\"oliver-twist-0-9.txt\") as l:\n",
    "    text = l.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:08:51.825531Z",
     "start_time": "2019-10-21T13:08:44.363572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n",
    "\n",
    "# Increase the allowable max lentgh\n",
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:08:53.482646Z",
     "start_time": "2019-10-21T13:08:51.845541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this function to seperate punctuations\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "tokens = separate_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:08:53.685765Z",
     "start_time": "2019-10-21T13:08:53.489634Z"
    }
   },
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 50 training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:05.508729Z",
     "start_time": "2019-10-21T13:08:55.061778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:11.318606Z",
     "start_time": "2019-10-21T13:09:05.556761Z"
    }
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:11.977035Z",
     "start_time": "2019-10-21T13:09:11.339604Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an LSTM based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:12.177456Z",
     "start_time": "2019-10-21T13:09:11.989036Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:12.355957Z",
     "start_time": "2019-10-21T13:09:12.185463Z"
    }
   },
   "outputs": [],
   "source": [
    "# create your own custom model\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim= seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:15.953250Z",
     "start_time": "2019-10-21T13:09:15.944244Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:19.608827Z",
     "start_time": "2019-10-21T13:09:19.397686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:09:23.146281Z",
     "start_time": "2019-10-21T13:09:23.136279Z"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:08:00.361885Z",
     "start_time": "2019-10-21T13:10:54.399824Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            109400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 150)           105600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4376)              660776    \n",
      "=================================================================\n",
      "Total params: 1,079,026\n",
      "Trainable params: 1,079,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = create_model(vocabulary_size+1, seq_len)\n",
    "# # fit model\n",
    "model.fit(X, y, batch_size=128, epochs=30,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:08:20.164596Z",
     "start_time": "2019-10-21T14:08:18.343386Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('epoch30.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('epoch30', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:08:24.242054Z",
     "start_time": "2019-10-21T14:08:24.223047Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:09:18.609506Z",
     "start_time": "2019-10-21T14:09:18.557470Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate (50 words in the video)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab a random seed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:23:13.788326Z",
     "start_time": "2019-10-21T14:23:13.772316Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(102)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T14:22:48.115187Z",
     "start_time": "2019-10-21T14:22:47.385707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyes in the white waistcoat which was a great deal and protege of the same end of the floor and the very old gentleman who had hitherto seen him into'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
